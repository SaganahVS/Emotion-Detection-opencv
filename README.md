

---

# ğŸ¥ Multimodal Emotion Detection System

This project implements a **Multimodal Emotion Detection System** that analyzes both **facial expressions** and **audio features** from video input to detect emotional states. It combines **computer vision**, **audio signal processing**, and **rule-based classification** for robust emotion prediction.

---

## ğŸ” Features

* ğŸ˜ **Facial Emotion Recognition** using DeepFace on sampled video frames
* ğŸ”Š **Audio Emotion Recognition** using MFCC, ZCR, RMSE, and spectral features via **Librosa**
* ğŸ”— **Multimodal Fusion** to combine facial and vocal emotion insights
* ğŸ“Š **Interactive Visualizations** using matplotlib
* ğŸï¸ **Video upload support** in Google Colab for end-to-end execution

---

## ğŸ§  How It Works

1. ğŸ“¥ **Upload a video** file via Google Colab
2. ğŸ§‘â€ğŸ¦± **Extract facial frames** and detect emotions using DeepFace
3. ğŸ§ **Extract and process audio** using spectral features (MFCC, ZCR, RMSE)
4. ğŸ§  **Classify audio emotion** using threshold-based logic
5. ğŸ”„ **Fuse** both facial and audio emotion cues for a final prediction
6. ğŸ“ˆ **Visualize results** with bar charts and audio spectrograms

---

## ğŸ“ Requirements

Make sure Python 3.x is installed and the following libraries are available:

```bash
pip install opencv-python librosa deepface moviepy matplotlib numpy
```

If running in **Google Colab**, also ensure:

```bash
pip install google-colab
```

### Additional Modules Used:

* `IPython.display` (for media output in Colab)

---

## ğŸš€ Getting Started

Run this project in **Google Colab** for best compatibility.

### Sample Usage:

```python
from your_script_name import MultimodalEmotionDetector

detector = MultimodalEmotionDetector()
detector.run_analysis()
```

> âš ï¸ Note: DeepFace will download pre-trained models on the first run, which may take some time.

---

## ğŸ“Š Output

The system provides the following outputs:

* ğŸ§  **Dominant facial emotions** detected over video frames
* ğŸ§ **Classified audio emotion** from extracted speech features
* ğŸ”„ **Final fused emotion** using combined audio-visual logic
* ğŸ“ˆ **Bar charts and spectrograms** for clear visualization of results

---

## ğŸ“Œ Use Case

This system is ideal for use in:

* ğŸ“ Affective computing research
* ğŸ’» Human-computer interaction studies
* ğŸ§ Behavioral analysis projects
* ğŸ¥ Multimedia emotion analytics

---

## ğŸ“„ License

This project is provided for **educational and research purposes only**.
Please refer to the licenses of individual libraries (e.g., DeepFace, Librosa) for commercial usage terms.

---

